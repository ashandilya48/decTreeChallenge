---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding

## Challenge Overview

**Your Mission:** Create a simple GitHub Pages site that demonstrates how decision trees measure feature importance and analyzes the critical differences between categorical and numerical variable encoding. You'll answer two key discussion questions by adding narrative to a pre-built analysis and posting those answers to your GitHub Pages site as a rendered HTML document.

## Discussion Questions for Challenge

**Your Task:** Add thoughtful narrative answers to these two questions in the Discussion Questions section of your rendered HTML site.

1. **Numerical vs Categorical Encoding:** There are two modelsin Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?  Is zipcode and ordinal or non-ordinal variable?

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case.  What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html) suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.

3. **Are There Any Suggestions for Implementing Decision Trees in Python With Prioper Categorical Handling?** Please poke around the Internet (AI is not as helpful with new libraries) for suggestions on how to implement decision trees in Python with better (i.e. not one-hot encoding) categorical handling.  Please provide a link to the source and a quote from the source.  There is not right answer here, but please provide a thoughtful answer, I am curious to see what you find.

*************

## Solution

::: {.callout-note}
## Numerical vs Categorical Encoding

The two models which were shared considered **ZipCode** as **`numerical`** variable and other as **`categorical`** variable. Based on my understanding of how zipcodes have a very significant effect on pricing of the real estate (Fun fact: Wife is a Real Estate Agent üòâ). 
For decision tree models, which are **non-parametric** and **insensitive** to monotonic transformations, the best way to encode zipcode depends on how you want the model to **interpret geographic information**.

### üß≠ Is Zip Code Ordinal or Non-Ordinal?

- **Zip code is a non-ordinal categorical variable.**
- Although zip codes are numeric, their values **do not imply order, magnitude, or proximity.**
  - For example, zip code 10001 (NYC) and 90210 (Beverly Hills) are far apart geographically, yet numerically close.
- They are **nominal identifiers** for geographic regions ‚Äî not ranked or scaled

### üß† How Should Zip Code Be Modeled?
‚úÖ **Categorically**, but with nuance depending on model type and goals

For **Decision Tree**:

- Label Encoding
- Target Encoding

Trees handle arbitrary labels well; ***target encoding*** can capture price patterns effectively.

:::

::: {.callout-note}
## R vs Python Implementation Differences

---
title: "Modeling Zip Code as a Categorical Variable: R vs Python"
format: html
editor: visual
---

## üß† Why Zip Code Modeling Differs Between R and Python

When modeling **zip code** as a categorical variable in decision trees, the resulting model structure and feature importance can differ significantly depending on whether you use **R** or **Python**. This discrepancy arises from how each language handles categorical data in tree-based algorithms.

### üîç Python (scikit-learn) Limitations

Python‚Äôs `scikit-learn` decision trees **do not natively support categorical variables**. Instead, categorical features like zip codes must be **manually encoded**, typically using:

- `OneHotEncoder`: creates binary columns for each category
- `OrdinalEncoder`: assigns integer codes to categories

This preprocessing step can distort the feature‚Äôs semantic meaning and inflate feature importance due to the **dimensionality explosion** from one-hot encoding.

> üìå **From scikit-learn documentation**:
> *"Decision trees do not support categorical variables natively. One-hot encoding is often used, but it can lead to less interpretable trees and biased splits."* 

### üß† R‚Äôs Native Categorical Support

In contrast, **R‚Äôs `rpart` and `CART` implementations** treat factors (categorical variables) **natively**. This means:

- No need for manual encoding
- Splits are made directly on category levels
- Trees remain interpretable and compact

This native handling allows R to **preserve the structure and meaning** of categorical variables like zip code, leading to more accurate and interpretable splits.

### üìä Impact on Tree Structure and Feature Importance

| Aspect                  | Python (scikit-learn)             | R (rpart/CART)                     |
|------------------------|-----------------------------------|-----------------------------------|
| Categorical support     | ‚ùå Manual encoding required        | ‚úÖ Native support for factors      |
| Tree interpretability   | ‚ö†Ô∏è Reduced due to encoding         | ‚úÖ Clear splits on categories      |
| Feature importance bias | ‚ö†Ô∏è Inflated by one-hot expansion   | ‚úÖ Balanced across true categories |
| Zip code modeling       | ‚ùå Risk of misleading splits       | ‚úÖ Semantically meaningful splits  |

### üèÜ Which Does a Better Job?

**R is generally better suited** for modeling zip code as a categorical variable in decision trees. Its native support for factors avoids the pitfalls of encoding and yields more interpretable and accurate models.

### üìö References

- [scikit-learn Decision Tree Documentation](https://scikit-learn.org/stable/modules/tree.html) 
- R `rpart` documentation: [https://cran.r-project.org/web/packages/rpart/rpart.pdf](https://cran.r-project.org/web/packages/rpart/rpart.pdf)

:::

::: {.callout-note}
## Are There Any Suggestions for Implementing Decision Trees in Python With Prioper Categorical Handling?

### üå≥ Why Categorical Handling Matters in Decision Trees
While decision trees theoretically support categorical splits, many implementations (like scikit-learn) require **manual encoding**, which can distort split logic:

- **Label encoding** imposes ordinal relationships where none exist.
- **One-hot encoding** increases dimensionality and can fragment splits.

For clean, native handling, you want libraries that:

- Accept raw categorical features
- Optimize splits over category groupings
- Avoid preprocessing leakage

### üîß Libraries Beyond scikit-learn for Decision Trees

| **Library** | **Native Categorical Support** | **Tree Types** | **Comments** |
|---|---|---|---| 
| **CatBoost** | ‚úÖ Yes | Boosted Trees | Best-in-class for categorical data; handles high cardinality | 
| **LightGBM** | ‚úÖ Yes | Boosted Trees | Fast, supports categorical splits via `categorical_feature` | 
| **XGBoost** | ‚ö†Ô∏è Partial (v1.3+) | Boosted Trees | Experimental support; otherwise needs encoding | 
| **ChefBoost** | ‚úÖ Yes | ID3, C4.5, CART | Great for classical decision trees with categorical splits | 
| **H2O.ai** | ‚úÖ Yes | GBM, Random Forest | Distributed, scalable, supports categorical natively | 
| **PySpark MLlib** | ‚úÖ Yes | Decision Trees | Good for large-scale categorical data in distributed settings | 
| **Custom CART/C4.5** | ‚úÖ Full control | Any | Ideal for research or tailored splitting logic | 

:::